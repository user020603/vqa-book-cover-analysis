{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SifCC78KMGq",
        "outputId": "cf6545ed-d909-4096-ed79-6339c55a1edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 1. Install the required libraries (uncomment in Colab if needed)\n",
        "# ---------------------------------------------\n",
        "!pip install transformers pillow torch torchvision pyngrok tensorboardX\n",
        "!pip install flask-ngrok\n",
        "!pip install flask-cors\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "nF5i9kRrKSbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd28450-012d-40d4-f090-021461434992",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (4.25.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=0.9->flask-cors) (3.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "95iUnKlBKWA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorBoard\n",
        "from tensorboardX import SummaryWriter"
      ],
      "metadata": {
        "id": "s2g9YrldKeMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 2. Define the Dataset\n",
        "# ---------------------------------------------\n",
        "class BookCoverDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        \"\"\"\n",
        "        data: list of dicts: {'image_path', 'title', 'author', 'publisher'}\n",
        "        tokenizer: a HuggingFace tokenizer\n",
        "        max_length: max sequence length for text fields\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()  # shape: [3, 224, 224]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # Tokenize each field\n",
        "        title_tokens = self.tokenizer(\n",
        "            item['title'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )['input_ids'].squeeze(0)\n",
        "\n",
        "        author_tokens = self.tokenizer(\n",
        "            item['author'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )['input_ids'].squeeze(0)\n",
        "\n",
        "        publisher_tokens = self.tokenizer(\n",
        "            item['publisher'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )['input_ids'].squeeze(0)\n",
        "\n",
        "        # Load and transform the image\n",
        "        image = Image.open(item['image_path']).convert('RGB')\n",
        "        image_tensor = self.image_transform(image)  # [3, 224, 224]\n",
        "\n",
        "        return {\n",
        "            'image': image_tensor,\n",
        "            'title_tokens': title_tokens,\n",
        "            'author_tokens': author_tokens,\n",
        "            'publisher_tokens': publisher_tokens\n",
        "        }"
      ],
      "metadata": {
        "id": "c_csacM1Knmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 3. Load JSON Data\n",
        "# ---------------------------------------------\n",
        "with open('drive/MyDrive/json_book/book_data.json', 'r') as file:\n",
        "    custom_data = json.load(file)"
      ],
      "metadata": {
        "id": "xqdfJbfOKqzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 4. Set Device\n",
        "# ---------------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEMZAKfyKstf",
        "outputId": "ad119910-14ad-40b5-d342-26aec616d465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 5. Load Base Model and Tokenizer\n",
        "# ---------------------------------------------\n",
        "base_model_name = 'openbmb/MiniCPM-Llama3-V-2_5-int4'\n",
        "base_model = AutoModel.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "base_model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "93f9530311be4c45b92c944cde97164f",
            "04bdfa16028a4b978270f588005e5a3e",
            "443baac6c7164fee92a1102f6879fff2",
            "3042a338593f475b9b4da570e7357adb",
            "e9a8cbcffa224e81924d382b0d5f3b94",
            "ec9ec9b5e92a4e48b86610419ce00a5b",
            "73fa3c25e74040ee9c00432b1e5138d8",
            "65f7deb512c54362b67f0012feb3cf94",
            "1d1c04bb0c4d4987bd36e27f4606e151",
            "5f6c46b40f374f7f8ff825de359c16b4",
            "0a6164c6762143018f7dbe5070ee9f0e"
          ]
        },
        "id": "Jn4KgUnwKvbr",
        "outputId": "a4a1c369-0c41-4810-ef18-be98ea62dc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93f9530311be4c45b92c944cde97164f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 6. Create Dataset and DataLoader\n",
        "# ---------------------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, temp_data = train_test_split(custom_data, test_size=0.2, random_state=42)  # 80% train\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)      # 10% val, 10% test\n",
        "\n",
        "print(\"length train\", len(train_data))\n",
        "print(\"length val:\", len(val_data))\n",
        "print(\"length test:\", len(test_data))\n",
        "\n",
        "train_dataset = BookCoverDataset(train_data, tokenizer)\n",
        "val_dataset = BookCoverDataset(val_data, tokenizer)\n",
        "test_dataset = BookCoverDataset(test_data, tokenizer)"
      ],
      "metadata": {
        "id": "KP6juuvtICLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518246ab-1fe4-4fab-a2f4-795561a8b6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length train 72\n",
            "length val: 9\n",
            "length test: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
      ],
      "metadata": {
        "id": "agVPlQZ0IrfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 7. Define Custom Model\n",
        "# ---------------------------------------------\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.title_head = nn.Linear(self.base_model.config.hidden_size, tokenizer.vocab_size)\n",
        "        self.author_head = nn.Linear(self.base_model.config.hidden_size, tokenizer.vocab_size)\n",
        "        self.publisher_head = nn.Linear(self.base_model.config.hidden_size, tokenizer.vocab_size)\n",
        "\n",
        "        # We'll map each pixel from 3 -> 128 dimension\n",
        "        self.image_transform = nn.Linear(3, 128)\n",
        "\n",
        "    def forward(self, image, title_ids, author_ids, publisher_ids):\n",
        "        \"\"\"\n",
        "        image: [batch_size, 3, 224, 224]\n",
        "        title_ids, author_ids, publisher_ids: [batch_size, seq_len]\n",
        "        \"\"\"\n",
        "        bsz, channels, height, width = image.shape  # e.g. [2, 3, 224, 224]\n",
        "\n",
        "        # Flatten to [bsz, height*width, 3]\n",
        "        image = image.permute(0, 2, 3, 1).contiguous()  # [2, 224, 224, 3]\n",
        "        image = image.view(bsz, -1, channels)           # [2, 50176, 3]\n",
        "\n",
        "        # Transform => [bsz, 50176, 128]\n",
        "        image_emb = self.image_transform(image)\n",
        "\n",
        "        # -------------------------------------------------------------\n",
        "        # IMPORTANT: We must pass pixel_values as a \"list of lists\"\n",
        "        # each sub-list = images for a single sample\n",
        "        # each item in sub-list = shape [seq_len, 128]\n",
        "        # So for bsz=2, we do:\n",
        "        # pixel_values = [\n",
        "        #   [ image_emb[0] ],  # shape: [50176, 128]\n",
        "        #   [ image_emb[1] ]\n",
        "        # ]\n",
        "        # Then the model code won't break on permute\n",
        "        # -------------------------------------------------------------\n",
        "        pixel_values_list = []\n",
        "        for i in range(bsz):\n",
        "            # image_emb[i] is shape [50176, 128]\n",
        "            # We wrap it in a list, so we get a sub-list of length 1\n",
        "            pixel_values_list.append([image_emb[i]])\n",
        "\n",
        "        # We'll use a typical \"data\" dict\n",
        "        tgt_sizes = torch.tensor([128], device=image.device)  # dummy\n",
        "\n",
        "        # Title\n",
        "        title_data = {\n",
        "            \"input_ids\": title_ids,          # [bsz, seq_len]\n",
        "            \"pixel_values\": pixel_values_list,  # list of lists\n",
        "            \"tgt_sizes\": tgt_sizes\n",
        "        }\n",
        "        title_outs = self.base_model(data=title_data)\n",
        "        # shape: title_outs.last_hidden_state => [bsz, seq_len, hidden_size]\n",
        "        title_logits = self.title_head(title_outs.last_hidden_state[:, 0, :])  # [bsz, vocab_size]\n",
        "\n",
        "        # Author\n",
        "        author_data = {\n",
        "            \"input_ids\": author_ids,\n",
        "            \"pixel_values\": pixel_values_list,\n",
        "            \"tgt_sizes\": tgt_sizes\n",
        "        }\n",
        "        author_outs = self.base_model(data=author_data)\n",
        "        author_logits = self.author_head(author_outs.last_hidden_state[:, 0, :])  # [bsz, vocab_size]\n",
        "\n",
        "        # Publisher\n",
        "        publisher_data = {\n",
        "            \"input_ids\": publisher_ids,\n",
        "            \"pixel_values\": pixel_values_list,\n",
        "            \"tgt_sizes\": tgt_sizes\n",
        "        }\n",
        "        publisher_outs = self.base_model(data=publisher_data)\n",
        "        publisher_logits = self.publisher_head(publisher_outs.last_hidden_state[:, 0, :])  # [bsz, vocab_size]\n",
        "\n",
        "        return title_logits, author_logits, publisher_logits\n",
        "\n",
        "model = CustomModel(base_model).to(device)"
      ],
      "metadata": {
        "id": "W2T6n4bHK952"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 8. Define Loss & Optimizer\n",
        "# ---------------------------------------------\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_loss(title_out, author_out, publisher_out,\n",
        "                 title_labels, author_labels, publisher_labels):\n",
        "    \"\"\"\n",
        "    Each out is [batch_size, vocab_size].\n",
        "    Each label is [batch_size].\n",
        "    If your labels are [batch_size, seq_len], you'll need a different approach.\n",
        "    \"\"\"\n",
        "    title_loss = loss_fn(title_out.view(-1, tokenizer.vocab_size), title_labels.view(-1))\n",
        "    author_loss = loss_fn(author_out.view(-1, tokenizer.vocab_size), author_labels.view(-1))\n",
        "    publisher_loss = loss_fn(publisher_out.view(-1, tokenizer.vocab_size), publisher_labels.view(-1))\n",
        "    return title_loss + author_loss + publisher_loss\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "f3AT46Z5LAPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 9. Quick Shape Check\n",
        "# ---------------------------------------------\n",
        "sample = train_dataset[0]\n",
        "print(\"[Sample 0] Title tokens:\", sample['title_tokens'].shape)\n",
        "print(\"[Sample 0] Image shape:\", sample['image'].shape)"
      ],
      "metadata": {
        "id": "tJnTtKeNLBj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4ff0eb-ce82-4636-9553-5006a6863878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sample 0] Title tokens: torch.Size([128])\n",
            "[Sample 0] Image shape: torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 10. Training Loop\n",
        "# ---------------------------------------------\n",
        "writer = SummaryWriter(log_dir=\"./runs\")\n",
        "num_epochs = 2\n",
        "best_val_loss = float(\"inf\")  # best val loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    # ---------- TRAINING LOOP ----------\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        images = batch['image'].to(device)\n",
        "        title_ids = batch['title_tokens'].to(device)\n",
        "        author_ids = batch['author_tokens'].to(device)\n",
        "        publisher_ids = batch['publisher_tokens'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        title_out, author_out, publisher_out = model(\n",
        "            image=images,\n",
        "            title_ids=title_ids,\n",
        "            author_ids=author_ids,\n",
        "            publisher_ids=publisher_ids\n",
        "        )\n",
        "\n",
        "        title_labels = title_ids[:, 0]\n",
        "        author_labels = author_ids[:, 0]\n",
        "        publisher_labels = publisher_ids[:, 0]\n",
        "\n",
        "        loss = compute_loss(\n",
        "            title_out, author_out, publisher_out,\n",
        "            title_labels, author_labels, publisher_labels\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        global_step = epoch * len(train_loader) + step\n",
        "        writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
        "\n",
        "        if (step+1) % 10 == 0:\n",
        "            print(f\"[Train] Epoch [{epoch+1}/{num_epochs}], step {step+1}/{len(train_loader)}, loss = {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] TRAIN loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ---------- VALIDATION LOOP ----------\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            title_ids = batch['title_tokens'].to(device)\n",
        "            author_ids = batch['author_tokens'].to(device)\n",
        "            publisher_ids = batch['publisher_tokens'].to(device)\n",
        "\n",
        "            title_out, author_out, publisher_out = model(\n",
        "                image=images,\n",
        "                title_ids=title_ids,\n",
        "                author_ids=author_ids,\n",
        "                publisher_ids=publisher_ids\n",
        "            )\n",
        "\n",
        "            title_labels = title_ids[:, 0]\n",
        "            author_labels = author_ids[:, 0]\n",
        "            publisher_labels = publisher_ids[:, 0]\n",
        "\n",
        "            val_loss = compute_loss(\n",
        "                title_out, author_out, publisher_out,\n",
        "                title_labels, author_labels, publisher_labels\n",
        "            )\n",
        "            running_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] VAL loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(\">>> Best model saved (val_loss improved).\")\n",
        "\n",
        "print(\"Training done!\")"
      ],
      "metadata": {
        "id": "HafxKsQcLE8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93f9530311be4c45b92c944cde97164f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04bdfa16028a4b978270f588005e5a3e",
              "IPY_MODEL_443baac6c7164fee92a1102f6879fff2",
              "IPY_MODEL_3042a338593f475b9b4da570e7357adb"
            ],
            "layout": "IPY_MODEL_e9a8cbcffa224e81924d382b0d5f3b94"
          }
        },
        "04bdfa16028a4b978270f588005e5a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec9ec9b5e92a4e48b86610419ce00a5b",
            "placeholder": "​",
            "style": "IPY_MODEL_73fa3c25e74040ee9c00432b1e5138d8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "443baac6c7164fee92a1102f6879fff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f7deb512c54362b67f0012feb3cf94",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d1c04bb0c4d4987bd36e27f4606e151",
            "value": 2
          }
        },
        "3042a338593f475b9b4da570e7357adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f6c46b40f374f7f8ff825de359c16b4",
            "placeholder": "​",
            "style": "IPY_MODEL_0a6164c6762143018f7dbe5070ee9f0e",
            "value": " 2/2 [00:28&lt;00:00, 13.06s/it]"
          }
        },
        "e9a8cbcffa224e81924d382b0d5f3b94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec9ec9b5e92a4e48b86610419ce00a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73fa3c25e74040ee9c00432b1e5138d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65f7deb512c54362b67f0012feb3cf94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1c04bb0c4d4987bd36e27f4606e151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f6c46b40f374f7f8ff825de359c16b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6164c6762143018f7dbe5070ee9f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}